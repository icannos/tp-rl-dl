\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{random\PYGZus{}policy}\PYG{p}{():}
	\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{	Return a random walk of the agent, taking uniformly each possible action}
\PYG{l+s+sd}{	\PYGZdq{}\PYGZdq{}\PYGZdq{}}
	\PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5000}\PYG{p}{)]}		
		
\PYG{k}{def} \PYG{n+nf}{staticbest\PYGZus{}policy}\PYG{p}{(}\PYG{n}{click\PYGZus{}rates}\PYG{p}{):}
	\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{	Takes the action which maximises the score on that trajectory}
\PYG{l+s+sd}{	\PYGZdq{}\PYGZdq{}\PYGZdq{}}
	\PYG{n}{a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{click\PYGZus{}rates}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{))}
	\PYG{k}{return} \PYG{p}{[}\PYG{n}{a} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5000}\PYG{p}{)]}
		
		
\PYG{k}{def} \PYG{n+nf}{opt\PYGZus{}policy}\PYG{p}{(}\PYG{n}{click\PYGZus{}rates}\PYG{p}{):}
	\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{	At each timestep takes the best action}
\PYG{l+s+sd}{	\PYGZdq{}\PYGZdq{}\PYGZdq{}}
	\PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{click\PYGZus{}rates}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{Verbatim}
